#!/usr/bin/env node

/**
 * GitHub Crawler Integration Test
 * æµ‹è¯• GitHub çˆ¬è™«çš„å®Œæ•´æµç¨‹ï¼šæŠ“å– â†’ å¤„ç† â†’ å…¥åº“
 */

require('dotenv').config()
const { createClient } = require('@supabase/supabase-js')

async function testGitHubCrawler() {
  console.log('ğŸ§ª GitHub Crawler Integration Test\n')
  console.log('='.repeat(60))
  
  // æ£€æŸ¥ç¯å¢ƒå˜é‡
  console.log('\nğŸ“‹ Environment Check:')
  console.log(`   GITHUB_TOKEN: ${process.env.GITHUB_TOKEN ? 'âœ“ Set' : 'âœ— Not set (will use unauthenticated API)'}`)
  console.log(`   SUPABASE_URL: ${process.env.NEXT_PUBLIC_SUPABASE_URL ? 'âœ“ Set' : 'âœ— Not set'}`)
  console.log(`   SUPABASE_KEY: ${process.env.SUPABASE_SERVICE_ROLE_KEY ? 'âœ“ Set' : 'âœ— Not set'}`)
  console.log(`   OPENAI_API_KEY: ${process.env.OPENAI_API_KEY ? 'âœ“ Set' : 'âœ— Not set'}`)
  
  if (!process.env.NEXT_PUBLIC_SUPABASE_URL || !process.env.SUPABASE_SERVICE_ROLE_KEY) {
    console.error('\nâŒ Missing required Supabase credentials')
    process.exit(1)
  }
  
  // åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
  const supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY
  )
  
  // è®°å½•çˆ¬å–å‰çš„æ•°æ®åº“çŠ¶æ€
  console.log('\nğŸ“Š Database State (Before):')
  const { data: beforeAgents, error: beforeError } = await supabase
    .from('agents')
    .select('id, name, source, github_stars')
    .eq('source', 'GitHub')
  
  if (beforeError) {
    console.error('âŒ Failed to query database:', beforeError)
    process.exit(1)
  }
  
  const beforeCount = beforeAgents?.length || 0
  console.log(`   GitHub Agents: ${beforeCount}`)
  
  // è¿è¡Œçˆ¬è™«ï¼ˆé™åˆ¶ä¸º 10 ä¸ªé¡¹ç›®ä»¥åŠ å¿«æµ‹è¯•ï¼‰
  console.log('\nğŸš€ Running GitHub Crawler...')
  console.log('   (Crawling 10 projects for testing)\n')
  
  const maxResults = parseInt(process.env.TEST_MAX_RESULTS || '10')
  
  // è®¾ç½®ç¯å¢ƒå˜é‡å¹¶è¿è¡Œçˆ¬è™«
  process.env.CRAWLER_SOURCE = 'github'
  process.env.CRAWLER_MAX_AGENTS_PER_RUN = maxResults.toString()
  process.env.GITHUB_MIN_STARS = '50' // æé«˜ stars è¦æ±‚ä»¥è·å–é«˜è´¨é‡é¡¹ç›®
  
  try {
    // åŠ¨æ€å¯¼å…¥çˆ¬è™«æ¨¡å—
    const { crawlAndExport } = require('../crawler/sources/github')
    const { batchEnrichAgents } = require('../crawler/enricher')
    
    // çˆ¬å–æ•°æ®
    const rawAgents = await crawlAndExport({
      topic: process.env.GITHUB_TOPIC || 'ai-agent',
      minStars: parseInt(process.env.GITHUB_MIN_STARS || '50'),
      maxResults
    })
    
    console.log(`\nâœ… Crawled ${rawAgents.length} repositories`)
    
    if (rawAgents.length === 0) {
      console.log('\nâš ï¸  No repositories found. This might be due to:')
      console.log('   - GitHub API rate limiting (try setting GITHUB_TOKEN)')
      console.log('   - No repositories matching the criteria')
      console.log('\nâœ“ Test completed (no data to process)')
      return
    }
    
    // æ˜¾ç¤ºå‰ 3 ä¸ªé¡¹ç›®çš„ä¿¡æ¯
    console.log('\nğŸ“¦ Sample Projects:')
    rawAgents.slice(0, 3).forEach((agent, i) => {
      console.log(`   ${i + 1}. ${agent.name}`)
      console.log(`      URL: ${agent.url}`)
      console.log(`      Stars: ${agent.github_stars || 0}`)
      console.log(`      Topics: ${agent.github_topics?.join(', ') || 'none'}`)
    })
    
    // AI åˆ†æå¹¶ä¿å­˜åˆ°æ•°æ®åº“
    console.log('\nğŸ¤– Enriching with AI analysis...')
    const result = await batchEnrichAgents(rawAgents)
    
    console.log('\nğŸ“Š Enrichment Results:')
    console.log(`   Created: ${result.created}`)
    console.log(`   Updated: ${result.updated}`)
    console.log(`   Failed: ${result.failed}`)
    
    // éªŒè¯æ•°æ®åº“çŠ¶æ€
    console.log('\nğŸ“Š Database State (After):')
    const { data: afterAgents, error: afterError } = await supabase
      .from('agents')
      .select('id, name, source, github_stars, github_url, github_topics')
      .eq('source', 'GitHub')
      .order('github_stars', { ascending: false })
      .limit(5)
    
    if (afterError) {
      console.error('âŒ Failed to query database:', afterError)
      process.exit(1)
    }
    
    const afterCount = afterAgents?.length || 0
    console.log(`   GitHub Agents: ${afterCount} (${afterCount - beforeCount > 0 ? '+' : ''}${afterCount - beforeCount})`)
    
    // æ˜¾ç¤ºå‰ 5 ä¸ª GitHub agents
    if (afterAgents && afterAgents.length > 0) {
      console.log('\nğŸ† Top GitHub Agents (by stars):')
      afterAgents.forEach((agent, i) => {
        console.log(`   ${i + 1}. ${agent.name}`)
        console.log(`      Stars: ${agent.github_stars || 0}`)
        console.log(`      URL: ${agent.github_url || 'N/A'}`)
        console.log(`      Topics: ${agent.github_topics?.join(', ') || 'none'}`)
      })
    }
    
    // éªŒè¯æ•°æ®å®Œæ•´æ€§
    console.log('\nâœ… Data Integrity Check:')
    const { data: integrityCheck } = await supabase
      .from('agents')
      .select('id, name, github_stars, github_url, github_owner')
      .eq('source', 'GitHub')
      .is('github_url', null)
    
    if (integrityCheck && integrityCheck.length > 0) {
      console.log(`   âš ï¸  Found ${integrityCheck.length} GitHub agents without github_url`)
    } else {
      console.log('   âœ“ All GitHub agents have required fields')
    }
    
    console.log('\n' + '='.repeat(60))
    console.log('âœ… GitHub Crawler Test Completed Successfully!')
    console.log('='.repeat(60))
    
  } catch (error) {
    console.error('\nâŒ Test failed:', error)
    console.error('\nStack trace:', error.stack)
    process.exit(1)
  }
}

// è¿è¡Œæµ‹è¯•
testGitHubCrawler()
