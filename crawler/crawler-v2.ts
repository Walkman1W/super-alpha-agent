#!/usr/bin/env node

/**
 * çˆ¬è™« V2 - ä¼˜åŒ–ç‰ˆæœ¬
 * 
 * ç‰¹æ€§:
 * 1. æ•°æ®åº“å»é‡ - æœç´¢å‰æ£€æŸ¥å·²å­˜åœ¨çš„agents
 * 2. å¹¶è¡Œå¤„ç† - ä½¿ç”¨é˜Ÿåˆ—å¹¶è¡Œå¤„ç†å¤šä¸ªagents
 * 3. å®æ—¶å­˜å‚¨ - å¤„ç†å®Œä¸€ä¸ªç«‹å³å­˜å‚¨ï¼Œå‰ç«¯å¯å®æ—¶çœ‹åˆ°
 * 4. å¤šTopicæ”¯æŒ - å¯é…ç½®å¤šä¸ªtopicè½®æ¢æŠ“å–
 * 5. å®¹é”™æœºåˆ¶ - ä¸­æ–­åå¯ç»§ç»­ï¼Œå·²å¤„ç†çš„æ•°æ®ä¸ä¸¢å¤±
 */

import { config } from 'dotenv'
config()

import { searchRepos } from '@/lib/github'
import { supabaseAdmin } from '@/lib/supabase'
import { enrichAndSaveAgent } from './enricher'
import type { ExtendedRawAgentData } from './enricher'

// é…ç½®æ¥å£
interface CrawlerConfig {
  topics: string[]           // è¦æŠ“å–çš„topicsåˆ—è¡¨
  minStars: number          // æœ€å°æ˜Ÿæ ‡æ•°
  maxAgentsPerTopic: number // æ¯ä¸ªtopicæœ€å¤šæŠ“å–æ•°é‡
  concurrency: number       // å¹¶å‘å¤„ç†æ•°é‡
  delayBetweenBatches: number // æ‰¹æ¬¡é—´å»¶è¿Ÿ(ms)
}

// ç»Ÿè®¡ä¿¡æ¯
interface CrawlerStats {
  totalFound: number
  filtered: number
  queued: number
  processed: number
  created: number
  updated: number
  failed: number
  skipped: number
}

// ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
function getConfig(): CrawlerConfig {
  const topicsStr = process.env.CRAWLER_TOPICS || process.env.GITHUB_TOPIC || 'ai-agent'
  const topics = topicsStr.split(',').map(t => t.trim())
  
  return {
    topics,
    minStars: parseInt(process.env.GITHUB_MIN_STARS || '100'),
    maxAgentsPerTopic: parseInt(process.env.CRAWLER_MAX_AGENTS_PER_RUN || '50'),
    concurrency: parseInt(process.env.CRAWLER_CONCURRENCY || '3'),
    delayBetweenBatches: parseInt(process.env.CRAWLER_BATCH_DELAY || '2000')
  }
}

/**
 * ä»æ•°æ®åº“è·å–å·²å­˜åœ¨çš„GitHub URLs
 */
async function getExistingGitHubUrls(): Promise<Set<string>> {
  console.log('ğŸ“Š æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„agents...')
  
  const { data, error } = await supabaseAdmin
    .from('agents')
    .select('github_url, source_id')
    .or('source.eq.GitHub,source.eq.github')
  
  if (error) {
    console.error('âŒ è·å–å·²å­˜åœ¨agentså¤±è´¥:', error)
    return new Set()
  }
  
  const urls = new Set<string>()
  data?.forEach(agent => {
    if (agent.github_url) urls.add(agent.github_url)
    if (agent.source_id) urls.add(agent.source_id)
  })
  
  console.log(`   æ‰¾åˆ° ${urls.size} ä¸ªå·²å­˜åœ¨çš„agents`)
  return urls
}

/**
 * æœç´¢å¹¶è¿‡æ»¤GitHubä»“åº“
 */
async function searchAndFilter(
  topic: string,
  minStars: number,
  maxResults: number,
  existingUrls: Set<string>
): Promise<ExtendedRawAgentData[]> {
  console.log(`\nğŸ” æœç´¢Topic: ${topic} (>= ${minStars} stars)`)
  
  // æœç´¢GitHub
  const repos = await searchRepos({
    topic,
    minStars,
    maxResults,
    sort: 'stars',
    order: 'desc'
  })
  
  console.log(`   æ‰¾åˆ° ${repos.length} ä¸ªä»“åº“`)
  
  // è¿‡æ»¤å·²å­˜åœ¨çš„
  const newRepos = repos.filter(repo => {
    const url = repo.html_url
    return !existingUrls.has(url)
  })
  
  console.log(`   è¿‡æ»¤åå‰©ä½™ ${newRepos.length} ä¸ªæ–°ä»“åº“`)
  
  // è½¬æ¢ä¸ºRawAgentDataæ ¼å¼
  const rawAgents: ExtendedRawAgentData[] = newRepos.map(repo => ({
    name: repo.name,
    description: repo.description || '',
    url: repo.html_url,
    platform: 'GitHub',
    author: repo.owner.login,
    github_stars: repo.stargazers_count,
    github_url: repo.html_url,
    github_owner: repo.owner.login,
    github_topics: repo.topics
  }))
  
  return rawAgents
}

/**
 * å¹¶è¡Œå¤„ç†é˜Ÿåˆ—
 */
async function processQueue(
  queue: ExtendedRawAgentData[],
  concurrency: number,
  stats: CrawlerStats
): Promise<void> {
  console.log(`\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† (å¹¶å‘æ•°: ${concurrency})`)
  console.log(`   é˜Ÿåˆ—é•¿åº¦: ${queue.length}`)
  
  const processing: Promise<void>[] = []
  let index = 0
  
  async function processNext() {
    while (index < queue.length) {
      const currentIndex = index++
      const agent = queue[currentIndex]
      
      try {
        console.log(`\n[${currentIndex + 1}/${queue.length}] ğŸ“ å¤„ç†: ${agent.name}`)
        
        // å¤„ç†å¹¶ç«‹å³å­˜å‚¨
        const result = await enrichAndSaveAgent(agent)
        
        stats.processed++
        if (result?.action === 'created') {
          stats.created++
          console.log(`   âœ… å·²åˆ›å»º`)
        } else if (result?.action === 'updated') {
          stats.updated++
          console.log(`   ğŸ”„ å·²æ›´æ–°`)
        }
        
      } catch (error) {
        stats.failed++
        console.error(`   âŒ å¤„ç†å¤±è´¥:`, error instanceof Error ? error.message : error)
      }
    }
  }
  
  // å¯åŠ¨å¹¶å‘worker
  for (let i = 0; i < concurrency; i++) {
    processing.push(processNext())
  }
  
  // ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
  await Promise.all(processing)
}

/**
 * æ˜¾ç¤ºå®æ—¶è¿›åº¦
 */
function printProgress(stats: CrawlerStats) {
  const total = stats.queued
  const processed = stats.processed
  const percentage = total > 0 ? ((processed / total) * 100).toFixed(1) : '0.0'
  
  console.log(`\nğŸ“Š å®æ—¶è¿›åº¦: ${processed}/${total} (${percentage}%)`)
  console.log(`   âœ… åˆ›å»º: ${stats.created}`)
  console.log(`   ğŸ”„ æ›´æ–°: ${stats.updated}`)
  console.log(`   âŒ å¤±è´¥: ${stats.failed}`)
}

/**
 * ä¸»å‡½æ•°
 */
async function main() {
  console.log('ğŸ¤– Super Alpha Agent Crawler V2\n')
  console.log('='.repeat(60))
  
  const config = getConfig()
  const stats: CrawlerStats = {
    totalFound: 0,
    filtered: 0,
    queued: 0,
    processed: 0,
    created: 0,
    updated: 0,
    failed: 0,
    skipped: 0
  }
  
  console.log('âš™ï¸  é…ç½®:')
  console.log(`   Topics: ${config.topics.join(', ')}`)
  console.log(`   æœ€å°Stars: ${config.minStars}`)
  console.log(`   æ¯Topicæœ€å¤š: ${config.maxAgentsPerTopic}`)
  console.log(`   å¹¶å‘æ•°: ${config.concurrency}`)
  console.log('='.repeat(60))
  
  try {
    // 1. è·å–å·²å­˜åœ¨çš„agents
    const existingUrls = await getExistingGitHubUrls()
    
    // 2. æœç´¢æ‰€æœ‰topics
    const allAgents: ExtendedRawAgentData[] = []
    
    for (const topic of config.topics) {
      const agents = await searchAndFilter(
        topic,
        config.minStars,
        config.maxAgentsPerTopic,
        existingUrls
      )
      
      stats.totalFound += agents.length
      allAgents.push(...agents)
      
      // Topicé—´å»¶è¿Ÿï¼Œé¿å…APIé™æµ
      if (config.topics.length > 1) {
        await new Promise(resolve => setTimeout(resolve, config.delayBetweenBatches))
      }
    }
    
    stats.queued = allAgents.length
    
    if (allAgents.length === 0) {
      console.log('\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–°çš„agentséœ€è¦å¤„ç†')
      return
    }
    
    console.log(`\nğŸ“¦ æ€»è®¡æ‰¾åˆ° ${allAgents.length} ä¸ªæ–°agentså¾…å¤„ç†`)
    
    // 3. å¹¶è¡Œå¤„ç†é˜Ÿåˆ—
    await processQueue(allAgents, config.concurrency, stats)
    
    // 4. æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    console.log('\n' + '='.repeat(60))
    console.log('ğŸ‰ çˆ¬è™«å®Œæˆï¼')
    console.log('='.repeat(60))
    console.log(`ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:`)
    console.log(`   æœç´¢åˆ°: ${stats.totalFound}`)
    console.log(`   é˜Ÿåˆ—ä¸­: ${stats.queued}`)
    console.log(`   å·²å¤„ç†: ${stats.processed}`)
    console.log(`   âœ… åˆ›å»º: ${stats.created}`)
    console.log(`   ğŸ”„ æ›´æ–°: ${stats.updated}`)
    console.log(`   âŒ å¤±è´¥: ${stats.failed}`)
    console.log(`   æˆåŠŸç‡: ${stats.queued > 0 ? ((stats.processed / stats.queued) * 100).toFixed(1) : 0}%`)
    console.log('='.repeat(60))
    
  } catch (error) {
    console.error('\nğŸ’¥ çˆ¬è™«å¤±è´¥:', error)
    process.exit(1)
  }
}

// è¿è¡Œ
if (require.main === module) {
  main()
}

export { main as runCrawlerV2 }
