#!/usr/bin/env node

import { crawlGPTStore, getGPTStoreSeedData } from './sources/gpt-store'
import { crawlAndExport } from './sources/github'
import { batchEnrichAgents } from './enricher'

async function main() {
  console.log('ğŸ¤– Super Alpha Agent Crawler\n')
  
  const maxAgents = parseInt(process.env.CRAWLER_MAX_AGENTS_PER_RUN || '50')
  const source = process.env.CRAWLER_SOURCE || 'gpt-store' // 'gpt-store' | 'github' | 'all'
  
  try {
    let rawAgents: any[] = []
    
    if (source === 'github' || source === 'all') {
      console.log('ğŸ“¦ Crawling GitHub...\n')
      const githubAgents = await crawlAndExport({
        topic: process.env.GITHUB_TOPIC || 'ai-agent',
        minStars: parseInt(process.env.GITHUB_MIN_STARS || '10'),
        maxResults: maxAgents
      })
      rawAgents.push(...githubAgents)
    }
    
    if (source === 'gpt-store' || source === 'all') {
      console.log('ğŸª Crawling GPT Store...\n')
      let gptAgents = await crawlGPTStore(maxAgents)
      
      // å¦‚æœçˆ¬å–å¤±è´¥æˆ–æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨ç§å­æ•°æ®
      if (gptAgents.length < 5) {
        console.log('âš ï¸  Crawler returned few results, using seed data...')
        gptAgents = getGPTStoreSeedData()
      }
      
      rawAgents.push(...gptAgents)
    }
    
    if (rawAgents.length === 0) {
      console.log('âš ï¸  No agents found to process')
      return
    }
    
    // AI åˆ†æå¹¶ä¿å­˜
    const result = await batchEnrichAgents(rawAgents)
    
    console.log('ğŸ‰ Crawler completed successfully!')
    console.log(`   Created: ${result.created}`)
    console.log(`   Updated: ${result.updated}`)
    console.log(`   Failed: ${result.failed}`)
    
  } catch (error) {
    console.error('ğŸ’¥ Crawler failed:', error)
    process.exit(1)
  }
}

main()
